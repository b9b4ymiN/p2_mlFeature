"""
Test Production-Ready Features

Tests all new production features:
1. Schema Validation
2. Data Alignment
3. Feature Versioning
4. Preprocessing/Scaling
5. Artifact Management

Run: python test_production_features.py
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import sys

print("="*80)
print("üß™ TESTING PRODUCTION-READY FEATURES")
print("="*80)

# ========================================
# Test 1: Schema Validation
# ========================================
print("\n[TEST 1] Schema Validation")
print("-"*80)

try:
    from schemas import OHLCV_SCHEMA, validate_all_feeds, print_validation_report

    # Create mock data
    dates = pd.date_range('2024-01-01', periods=100, freq='5min', tz='UTC')

    mock_ohlcv = pd.DataFrame({
        'timestamp': dates,
        'open': np.random.uniform(40000, 41000, 100),
        'high': np.random.uniform(40000, 41000, 100),
        'low': np.random.uniform(40000, 41000, 100),
        'close': np.random.uniform(40000, 41000, 100),
        'volume': np.random.uniform(100, 1000, 100)
    })

    # Validate
    result = OHLCV_SCHEMA.validate(mock_ohlcv)

    if result['valid']:
        print("‚úÖ Schema validation: PASS")
    else:
        print("‚ùå Schema validation: FAIL")
        print(f"   Errors: {result['errors']}")

except Exception as e:
    print(f"‚ùå Schema validation: ERROR - {e}")
    sys.exit(1)

# ========================================
# Test 2: Data Alignment
# ========================================
print("\n[TEST 2] Data Alignment")
print("-"*80)

try:
    from utils.data_alignment import DataAligner

    # Create mock data with different frequencies
    dates_5min = pd.date_range('2024-01-01', periods=100, freq='5min', tz='UTC')
    dates_sparse = pd.date_range('2024-01-01', periods=50, freq='10min', tz='UTC')

    mock_ohlcv = pd.DataFrame({
        'timestamp': dates_5min,
        'open': np.random.uniform(40000, 41000, 100),
        'high': np.random.uniform(40000, 41000, 100),
        'low': np.random.uniform(40000, 41000, 100),
        'close': np.random.uniform(40000, 41000, 100),
        'volume': np.random.uniform(100, 1000, 100)
    })

    mock_oi = pd.DataFrame({
        'timestamp': dates_sparse,
        'open_interest': np.random.uniform(1e9, 2e9, 50)
    })

    # Align
    aligner = DataAligner(base_frequency='5min', timezone='UTC')
    aligned, report = aligner.align_and_resample(
        ohlcv=mock_ohlcv,
        oi=mock_oi,
        fill_method='ffill'
    )

    if len(aligned) == 100:
        print("‚úÖ Data alignment: PASS")
        print(f"   Aligned shape: {aligned.shape}")
    else:
        print("‚ùå Data alignment: FAIL")
        print(f"   Expected 100 rows, got {len(aligned)}")

except Exception as e:
    print(f"‚ùå Data alignment: ERROR - {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# ========================================
# Test 3: Feature Versioning
# ========================================
print("\n[TEST 3] Feature Versioning")
print("-"*80)

try:
    from utils.feature_versioning import save_feature_list, load_feature_list

    # Create test features
    test_features = [
        'oi_sma_20',
        'price_vs_vwap',
        'volume_sma_50',
        'rsi_14'
    ]

    test_config = {
        'windows': [20, 50],
        'test': True
    }

    # Save
    feature_set_id = save_feature_list(
        feature_names=test_features,
        config=test_config,
        description="Test feature set"
    )

    # Load
    loaded_features, metadata = load_feature_list(feature_set_id)

    if loaded_features == sorted(test_features):
        print("‚úÖ Feature versioning: PASS")
        print(f"   Feature Set ID: {feature_set_id}")
    else:
        print("‚ùå Feature versioning: FAIL")
        print(f"   Expected: {sorted(test_features)}")
        print(f"   Got: {loaded_features}")

except Exception as e:
    print(f"‚ùå Feature versioning: ERROR - {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# ========================================
# Test 4: Preprocessing & Scaling
# ========================================
print("\n[TEST 4] Preprocessing & Scaling")
print("-"*80)

try:
    from models.preprocessing import scale_train_val_test

    # Create mock data
    np.random.seed(42)

    X_train = pd.DataFrame({
        'feat1': np.random.uniform(0, 100, 1000),
        'feat2': np.random.uniform(-50, 50, 1000)
    })

    X_val = pd.DataFrame({
        'feat1': np.random.uniform(0, 100, 200),
        'feat2': np.random.uniform(-50, 50, 200)
    })

    X_test = pd.DataFrame({
        'feat1': np.random.uniform(0, 100, 200),
        'feat2': np.random.uniform(-50, 50, 200)
    })

    # Scale
    X_train_s, X_val_s, X_test_s, scaler = scale_train_val_test(
        X_train, X_val, X_test,
        feature_set_id='test_scaling_123',
        scaler_type='standard'
    )

    # Check that train is normalized
    train_mean = X_train_s.mean().mean()
    train_std = X_train_s.std().mean()

    if abs(train_mean) < 0.1 and abs(train_std - 1.0) < 0.1:
        print("‚úÖ Preprocessing & Scaling: PASS")
        print(f"   Train mean: {train_mean:.6f} (should be ~0)")
        print(f"   Train std:  {train_std:.6f} (should be ~1)")
    else:
        print("‚ùå Preprocessing & Scaling: FAIL")
        print(f"   Train mean: {train_mean:.6f}")
        print(f"   Train std:  {train_std:.6f}")

except Exception as e:
    print(f"‚ùå Preprocessing & Scaling: ERROR - {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# ========================================
# Test 5: Artifact Management
# ========================================
print("\n[TEST 5] Artifact Management")
print("-"*80)

try:
    from utils.artifact_manager import export_prepared_datasets, load_prepared_datasets

    # Create mock data
    np.random.seed(42)

    X_train = pd.DataFrame({'feat1': np.random.randn(100)})
    y_train = pd.Series(np.random.randint(0, 3, 100))

    X_val = pd.DataFrame({'feat1': np.random.randn(20)})
    y_val = pd.Series(np.random.randint(0, 3, 20))

    X_test = pd.DataFrame({'feat1': np.random.randn(20)})
    y_test = pd.Series(np.random.randint(0, 3, 20))

    # Export
    test_id = 'test_artifacts_123'
    output_dir = export_prepared_datasets(
        X_train, y_train, X_val, y_val, X_test, y_test,
        feature_set_id=test_id,
        metadata={'test': True}
    )

    # Load
    X_train_l, y_train_l, X_val_l, y_val_l, X_test_l, y_test_l, meta = load_prepared_datasets(test_id)

    if (X_train.equals(X_train_l) and
        y_train.equals(y_train_l) and
        meta['feature_set_id'] == test_id):
        print("‚úÖ Artifact Management: PASS")
        print(f"   Exported to: {output_dir}")
    else:
        print("‚ùå Artifact Management: FAIL")
        print("   Data mismatch after load")

except Exception as e:
    print(f"‚ùå Artifact Management: ERROR - {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# ========================================
# FINAL SUMMARY
# ========================================
print("\n" + "="*80)
print("‚úÖ ALL TESTS PASSED!")
print("="*80)

print("\nüìã Summary:")
print("   ‚úÖ Schema Validation - Working")
print("   ‚úÖ Data Alignment - Working")
print("   ‚úÖ Feature Versioning - Working")
print("   ‚úÖ Preprocessing & Scaling - Working")
print("   ‚úÖ Artifact Management - Working")

print("\nüéâ Production-ready features are fully functional!")
print("="*80)
